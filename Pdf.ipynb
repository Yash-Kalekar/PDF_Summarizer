{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing necessary packages and importing the libraries"
      ],
      "metadata": {
        "id": "xQ06y-2GRMiX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SqiH3c1iRJ4m",
        "outputId": "4b4c0743-eece-4ecf-91a4-7be876450697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (23.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.10\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.3 [186 kB]\n",
            "Fetched 186 kB in 1s (221 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121749 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.3_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n",
            "Collecting tesseract\n",
            "  Downloading tesseract-0.1.3.tar.gz (45.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.6/45.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tesseract\n",
            "  Building wheel for tesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tesseract: filename=tesseract-0.1.3-py3-none-any.whl size=45562552 sha256=050ce0d479d54b359eeb60b63b92c0dc86ee52f17a1b9e14b6054efdda4551c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/71/c9/aa/698c579693e83fdda9ad6d6f0d8f61ed986e27925ef576f109\n",
            "Successfully built tesseract\n",
            "Installing collected packages: tesseract\n",
            "Successfully installed tesseract-0.1.3\n",
            "Collecting Pillow==9.0.0\n",
            "  Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "Successfully installed Pillow-9.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 1s (4,837 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 121779 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract\n",
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image\n",
        "!pip install tesseract\n",
        "!pip install Pillow==9.0.0\n",
        "!apt install tesseract-ocr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import"
      ],
      "metadata": {
        "id": "1NjetrTvRQ-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from pytesseract import image_to_string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from scipy import spatial\n",
        "import networkx as nx\n",
        "import nltk"
      ],
      "metadata": {
        "id": "VQhRQqebRThZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connection"
      ],
      "metadata": {
        "id": "UjChV0r9RVX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXdM9YAfRXbX",
        "outputId": "2cc0f0d0-3dfb-4aae-8732-9646a9d2c22e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " converting the scanned document to image"
      ],
      "metadata": {
        "id": "DXqy9fyqRb8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path of the pdf\n",
        "\n",
        "PDF_file = \"/content/drive/MyDrive/ML/greenit.pdf\"\n",
        "\n",
        "'''\n",
        "Module #1: Converting PDF to images\n",
        "'''\n",
        "\n",
        "# Store all the pages of the PDF in a variable\n",
        "pages = convert_from_path(\"/content/drive/MyDrive/ML/greenit.pdf\", 500)\n",
        "\n",
        "#Counter to store images of each page of PDF to image\n",
        "image_counter = 1\n",
        "\n",
        "# Iterate through all the pages stored above\n",
        "\n",
        "for page in pages:\n",
        "  filename = \"page_\" + str(image_counter) + \".jpg\"\n",
        "  page.save(filename, 'JPEG')\n",
        "  image_counter = image_counter + 1"
      ],
      "metadata": {
        "id": "PkqytFgCRdgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing OCR on the converted image"
      ],
      "metadata": {
        "id": "G_b4LHvUSq_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Variable to get count of total number of pages\n",
        "filelimit = image_counter - 1\n",
        "\n",
        "#Creating a text file to write the output\n",
        "outfile = \"green.txt\"\n",
        "\n",
        "#Open the file in append mode so that\n",
        "#All contents of all images are added to the same file\n",
        "f = open(outfile, \"a\")\n",
        "\n",
        "# Iterate from 1 to total number of pages\n",
        "for i in range(1, filelimit + 1):\n",
        "   filename = \"page_\" + str(i) + \".jpg\"\n",
        "   text = str(((pytesseract.image_to_string(Image.open(filename)))))\n",
        "   text = text.replace('-\\n', '')\n",
        "   f.write(text)\n",
        "\n",
        "#Close the file after writing all the text.\n",
        "f.close()"
      ],
      "metadata": {
        "id": "eRzcwIWdSrvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the TextRank Algorithm to perform text summarization"
      ],
      "metadata": {
        "id": "dW89VOwNTcnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o97CLJDpTdK9",
        "outputId": "c1b606cb-3538-44b9-9b91-9aa85c0e3095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sent_tokenize(text)"
      ],
      "metadata": {
        "id": "FIG0Lv55TrH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "sentence_tokens = [[word for word in sentence.split(' ') if word not in stop_words] for sentence in sentences_clean]"
      ],
      "metadata": {
        "id": "5oz22nalTxgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n",
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "amacgN7uUT0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed061c1-d04d-4a08-840e-b528a005d1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-24 11:43:50--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-02-24 11:43:50--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-02-24 11:43:50--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 38s  \n",
            "\n",
            "2024-02-24 11:46:29 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "metadata": {
        "id": "jjnqpNclUkem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_embeddings)"
      ],
      "metadata": {
        "id": "5Anv9PNZUy8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2892157d-918d-4164-9442-3ed0b911f3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400000"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuations, numbers and special characters\n",
        "clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n",
        "\n",
        "# make alphabets lowercase\n",
        "\n",
        "clean_sentences = [s.lower() for s in clean_sentences]"
      ],
      "metadata": {
        "id": "7xmrO0UqU2Gi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6444971-d901-4ee8-8b47-c2878e290f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-f94aa3d87f1d>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  clean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "8OwGYUF1VLXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove stopwords\n",
        "def remove_stopwords(sen):\n",
        "  sen_new = \" \".join([i for i in sen if i not in stop_words])\n",
        "  return sen_new\n",
        "# remove stopwords from the sentences\n",
        "clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]"
      ],
      "metadata": {
        "id": "A3jITlvLVf2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  word_embeddings[word] = coefs\n",
        "f.close()\n",
        "\n",
        "sentence_vectors = []\n",
        "for i in clean_sentences:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split()) +0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)"
      ],
      "metadata": {
        "id": "ZT3Wyj-uWBO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After extracting the word embeddings we form a similarity matrix by calculating the cosine similarity between each pair of sentences. After this is done, the matrix is then converted into graph with the nodes representing the sentences and the similarity representing the edges."
      ],
      "metadata": {
        "id": "iI-S9mLnWoZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#similarity matrix\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])"
      ],
      "metadata": {
        "id": "9SMizReKWpdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "nyzU1fQQW5dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i !=j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "metadata": {
        "id": "Mp6DrTQEXC-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "21wrl5UuXZpc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fcce85-7c81-4a46-9df2-33152e30d096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 0.0340658245685241, 1: 0.03555490550574928, 2: 0.03580357682202379, 3: 0.03404305498220462, 4: 0.0341671269019479, 5: 0.029286455576598282, 6: 0.034327750657352975, 7: 0.0341523657563127, 8: 0.03442386293176325, 9: 0.035862146258779995, 10: 0.03552233809744091, 11: 0.033101527273409016, 12: 0.0350821712654312, 13: 0.03638742831170936, 14: 0.033841042959568524, 15: 0.021807373621909404, 16: 0.03170074800961943, 17: 0.029190336221397467, 18: 0.029190794978414168, 19: 0.0347391897372993, 20: 0.032207321666540424, 21: 0.035205561724599606, 22: 0.02602627074313604, 23: 0.03719969706244466, 24: 0.017980833380062466, 25: 0.035157115727390026, 26: 0.004769533835018383, 27: 0.004769533835018383, 28: 0.004769533835018383, 29: 0.033588700518631116, 30: 0.025242654482203624, 31: 0.026274044494300156, 32: 0.03095314540088547, 33: 0.023606032857295445}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_sentence = {sentence:scores[index] for index, sentence in enumerate(sentences)}\n",
        "top = dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:4])"
      ],
      "metadata": {
        "id": "xDb6Bb3QXoW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing the Summary\n",
        "\n"
      ],
      "metadata": {
        "id": "G8oM8WWjX8tV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = \"\"\n",
        "for sent in sentences:\n",
        "  if sent in top.keys():\n",
        "    summary += str(sent)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "N48hit15X9hG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b769edb-212d-4e51-90eb-2f14d92789cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Along these lines, tools such as environmental Web portals, blogs, wikis, and interactive\n",
            "simulations of the environmental impact of an\n",
            "activity could offer assistance.However, going\n",
            "forward, the IT industry will need to deal with\n",
            "all of the infrastructure requirements and the\n",
            "environmental impact of IT and its use.Green IT Resources\n",
            "\n",
            "The following are some helpful electronic resources:\n",
            "\n",
            "> C/lO’s “Green IT” page (http://advice.cio.com/\n",
            "taxonomy/term/27/0) discusses how to stay out of\n",
            "the red with an environmental approach.cgi?m=newsletter) presents how businesses are\n",
            "examining how they use their resources and exploring ways to save energy, materials, and money with\n",
            "green il:\n",
            "\n",
            "> GreenBiz (www.greenbiz.com) is the leading information resource on how to align environmental\n",
            "responsibility with business success.\n"
          ]
        }
      ]
    }
  ]
}